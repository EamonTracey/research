{"system": ["You are a helpful TaskVine coding assistant. Provide strictly the requested code using the ndcctools.taskvine library.", "The TaskVine user manual is below:\nTaskVine User's Manual\nOverview\n\nTaskVine is a framework for building large scale data intensive dynamic workflows that run on high performance computing (HPC) clusters, GPU clusters, cloud service providers, and other distributed computing systems. A workflow is a collection of programs and files that are organized in a graph structure, allowing parts of the workflow to run in a parallel, reproducible way:\n\nA TaskVine workflow requires a manager and a large number of worker processes. The application generates a large number of small tasks, which are distributed to workers. As tasks access external data sources and produce their own outputs, more and more data is pulled into local storage on cluster nodes. This data is used to accelerate future tasks and avoid re-computing exisiting results. The application gradually grows \"like a vine\" through the cluster.\n\nThe TaskVine system is naturally robust. While an application is running, workers may be added or removed as computing resources become available. Newly added workers will gradually accumulate data within the cluster. Removed (or failed) workers are handled gracefully, and tasks will be retried elsewhere as needed. If a worker failure results in the loss of files, tasks will be re-executed as necessary to re-create them.\n\nTaskVine manager applications can be written in Python or C on Linux or OSX platforms. Individual tasks can be simple Python functions, complex Unix applications, or serverless function invocations. The key idea is that you declare file objects, and then declare tasks that consume them and produce new file objects. For example, this snippet draws an input file from the Project Gutenberg repository and runs a Task to search for the string \"needle\", producing the file output.txt:\n\nf = m.declare_url(\"https://www.gutenberg.org/cache/epub/2600/pg2600.txt\")\ng = m.declare_file(\"myoutput.txt\")\n\nt = Task(\"grep needle warandpeace.txt > output.txt\")\nt.add_input(f, \"warandpeace.txt\")\nt.add_output(g, \"outfile.txt\")\n\nTasks share a common set of options. Each task can be labelled with the resources (CPU cores, GPU devices, memory, disk space) that it needs to execute. This allows each worker to pack the appropriate number of tasks. For example, a worker running on a 64-core machine could run 32 dual-core tasks, 16 four-core tasks, or any other combination that adds up to 64 cores. If you don't know the resources needed, you can enable a resource monitor to automatically track, report, and allocate what each task uses.\n\nTaskVine is easy to deploy on existing HPC and cloud facilities. The worker processes are self-contained executables, and TaskVine arranges for all necessary task dependencies to be moved to workers, making the system self-hosting. Applications regularly consist of millions of tasks running on thousands of workers. Tools are provided to easily deploy workers on HTCondor, SLURM, and Grid Engine.\n\nTaskVine is our third-generation workflow system, built on our twenty years of experience creating scalable applications in fields such as high energy physics, bioinformatics, molecular dynamics, and machine learning.\nQuick Start\n\nInstalling via conda is the easiest method for most users. First, Install Miniforge if you don't already have conda installed. Then, open a terminal and install ndcctools like this:\n\nconda install -c conda-forge ndcctools\n\nUsing a text editor, create a manager program called quickstart.py like this:\n\n# quickstart.py\n\nimport ndcctools.taskvine as vine\n\n# Create a new manager\nm = vine.Manager([9123, 9129])\nprint(f\"Listening on port {m.port}\")\n\n# Declare a common input file to be shared by multiple tasks.\nf = m.declare_url(\"https://www.gutenberg.org/cache/epub/2600/pg2600.txt\", cache=\"workflow\")\n\n# Submit several tasks using that file.\nprint(\"Submitting tasks...\")\nfor keyword in ['needle', 'house', 'water']:\n    task = vine.Task(f\"grep {keyword} warandpeace.txt | wc\")\n    task.add_input(f, \"warandpeace.txt\")\n    task.set_cores(1)\n    m.submit(task)\n\n# As they complete, display the results:\nprint(\"Waiting for tasks to complete...\")\nwhile not m.empty():\n    task = m.wait(5)\n    if task:\n        print(f\"Task {task.id} completed with result {task.output}\")\n\nprint(\"All tasks done.\")\n\nRun the manager program at the command line like this:\n\npython quickstart.py\n\nIt should display output like this:\n\nListening on port 9123\nSubmitting tasks...\nWaiting for tasks to complete...\n\nThe manager is now waiting for workers to connect and begin requesting work. (Without any workers, nothing will happen.) You can start one worker on the same machine by opening a new shell and running:\n\nvine_worker localhost 9123\n\nThe manager will send tasks to the worker for execution. As they complete, you will see output like this:\n\nTask 1 completed with result      12     139     824\nTask 3 completed with result      99    1199    6672\nTask 2 completed with result     536    6314   36667\nAll tasks done.\n\nCongratulations! You have now run a simple manager application that runs tasks on one local worker. To scale up, simply run more workers on a cluster or cloud facility.\nExample Applications\n\nThe following examples show more complex applications and various features of TaskVine:\n\n    BLAST Example\n    Gutenberg Example\n    Mosaic Example\n    Gradient Descent Example\n    Watch Files Example\n    Functional Example\n\nRead on to learn how to build applications from scratch and run large numbers of workers at scale.\nWriting a TaskVine Application\n\nA TaskVine application can be written in Python, or C. In each language, the underlying principles are the same, but there are some syntactic differences shown below. The full API documentation for each language is here:\n\n    TaskVine Python API\n    TaskVine C API\n\nCreating a Manager Object\n\nTo begin, you must import the TaskVine module, and then create a Manager object. You may specify a specific port number to listen on like this:\nPython\n\n# Import the taskvine module\nimport ndcctools.taskvine as vine\n\n# Create a new manager listening on port 9123\nm = vine.Manager(9123)\n\nC\n\nIn a shared environment, that specific port might already be in use, and so you may find it more convenient to specify zero to indicated any available port:\nPython\n\n# Create a new manager listening on any port\nm = vine.Manager(0)\nprint(f\"listening on port {m.port}\")\n\nC\nDeclaring Files\n\nEach file used in a TaskVine application must be declared to the manager, so that it can be cached and replicated within the cluster as needed. The following are examples of basic file descriptions:\nPython\n\na = m.declare_file(\"mydata.txt\")\nd = m.declare_file(\"dataset/\")\nu = m.declare_url(\"https://ftp.ncbi.nlm.nih.gov/blast/db/human_genome.00.tar.gz\")\nb = m.declare_buffer(\"These words are the contents of the file.\")\nt = m.declare_temp()\n\nC\n\ndeclare_file indicates a file in the manager's local filesystem which will be transferred into the cluster and made available to tasks. Both files and directories can be declared in this way: declare_file(\"mydata.txt\") indicates a single text file, while declare_file(\"dataset\") refers to an entire directory tree. A local file or directory can also be used as the output of a task.\n\ndeclare_url indicates a remote dataset that will be loaded as needed into the cluster. This URL can be http, https, ftp, file or any other method supported by the standard curl tool. If many tasks need the same remote URL, TaskVine is careful to limit the number of concurrent transfers, and will seek to duplicate the file within the cluster, rather than subject the source to a denial-of-service attack.\n\ndeclare_buffer indicates a literal string of data that will be presented to that task in the form of a file. A buffer with no initial data can also be used as the output of a task, made easily visible within the manager application.\n\ndeclare_temp indicates an unnamed ephemeral file that can be used to capture the output of a task, and then serve as the input of a later task. Temporary files exist only within the cluster for the duration of a workflow, and are deleted when no longer needed. This accelerates a workflow by avoiding the step of returning the data to the manager. If a temporary file is unexpectedly lost due to the crash or failure of a worker, then the task that created it will be re-executed. Temp files may also be replicated across workers to a degree set by the vine_tune parameter temp-replica-count. Temp file replicas are useful if significant work is required to re-execute the task that created it. The contents of a temporary file can be obtained with fetch_file\n\nIf it is necessary to unpack a file before it is used, use the declare_untar transformation to wrap the file definition. This will permit the unpacked version to be shared by multiple tasks at once:\nPython\n\nu = m.declare_url(\"https://ftp.ncbi.nlm.nih.gov/blast/db/human_genome.00.tar.gz\")\nx = m.declare_untar(u)\n\nC\n\ndeclare_untar is an example of a MiniTask, which is explained further below.\nDeclaring Tasks\n\nTaskVine supports several forms of tasks: Standard Tasks consist of Unix command lines, Python Tasks consist of Python functions and their dependencies, and Serverless Tasks consist of invocations of functions in remote libraries.\n\nA Standard Task consists of a Unix command line to execute and the resources needed for that task. Previously declared input and output files must be attached to the task to provide it with the necessary data.\n\nHere is an example of a task that consists of the standard Unix gzip program, which will read the file mydata and produce mydata.gz as an output:\nPython\n\nt = vine.Task(\"gzip < mydata > mydata.gz\")\nt.add_input(a, \"mydata\")\nt.add_output(b, \"mydata.gz\")\n\nC\n\nNote that each task will execute in a private sandbox at a worker. And so, each input and output file must be \"attached\" to the task under a specific name. The task will only have access to those files specifically declared, and should not assume access to a general shared filesystem.\n\nWhen the task executes, the worker will create a sandbox directory, which serves as the working directory for the task. Each of the input files and directories will be linked into the sandbox directory with the given remote names. The task should write its outputs into the current working directory, which will be extracted from the locations given in the add_output statements.\n\nThe path of the sandbox directory is exported to the execution environment of each worker through the VINE_SANDBOX shell environment variable. This shell variable can be used in the execution environment of the worker to describe and access the locations of files in the sandbox directory.\n\nWarning\n\nThe remote names given go to the files should match the names in the command line of the task.\n\nIn Python you may find it more convenient to declare a task as a dictionary.\nPython\n\nt = vine.Task(\n    command = \"./gzip < mydata > mydata.gz\",\n    input_files = {\n        a : {\n            remote_name : \"mydata\",\n            cache : False\n        }\n    },\n    output_files = {\n        b : {\n            remote_name : \"mydata.gz\",\n            cache : False\n        }\n    }\n)\n\nIn addition to describing the input and output files, you may optionally specify additional details about the task that will assist TaskVine in making good scheduling decisions.\n\nIf you are able, describe the resources needed by each task (cores, gpus, memory, disk) so that the worker can pack as many concurrent tasks. This is described in greater detail under Managing Resources.\n\nYou may also attach a tag to a task, which is just a user-defined string that describes the purpose of the task. The tag is available as t.tag when the task is complete.\nPython\n\nt.set_cores(2)\nt.set_memory(4096)\nt.set_tag(\"config-4.5.0\")\n\n# this can once again be done at task declaration as well:\n t = vine.Task(\n    command = \"./gzip < my-file > my-file.gz\",\n    cores = 2,\n    memory = 4096,\n    tag = \"config-4.5.0\"\n )\n\nC\nManaging Tasks\n\nOnce a task has been fully specified, it can be submitted to the manager. submit returns a unique taskid that can be helpful when later referring to a task:\nPython\n\ntaskid = m.submit(t)\n\nC\n\nOnce all tasks are submitted, use wait to wait until a task completes, indicating how many seconds you are willing to pause. If a task completes within that time limit, then wait will return that task object. If no task completes within the timeout, it returns null.\nPython\n\nwhile not m.empty():\n    t = m.wait(5)\n    if t:\n        print(f\"Task {t.id} has returned!\")\n\n        if t.successful():\n            print(f\"stdout:\\n{t.std_output}\")\n        if t.completed():\n            print(f\"task complete with error exit code: {t.exit_code}\")\n        else:\n            print(f\"There was a problem executing the task: {t.result}\")\n\nC\n\nA completed task will have its output files written to disk. You may examine the standard output of the task in output and the exit code in exit_status.\n\nNote\n\nThe size of standard output is limited to 1 GB. Any output beyond 1 GB will be truncated. If large output is expected, redirect the stdout ./my-command > my-stdout of the task to a file and specify the file as an output file of the task as described above.\n\nWhen you are done with the task, delete it (only needed for C):\nC\n\nvine_task_delete(t);\n\nContinue submitting and waiting for tasks until all work is complete. You may check to make sure that the manager is empty with vine_empty. When all is done, delete the manager (only needed for C):\nC\n\nvine_delete(m);\n\nFull details of all of the TaskVine functions can be found in the TaskVine API.\nRunning a TaskVine Application\n\nThere are a variety of ways to execute a TaskVine application at scale. The examples in this section make use of the example program functions.py which you can download to execute like this:\n\npython3 functions.py\n\nLanguage Specific Setup\n\nBefore running the application, you may need some additional setup, depending on the language in use:\nPython Setup\n\nIf you installed via Conda, then no further setup is needed.\n\nIf you are running a Python application and did not install via Conda, then you will need to set the PYTHONPATH to point to the cctools installation, like this:\n\n# Note: This is only needed if not using Conda:\n$ PYVER=$(python -c 'import sys; print(\"%s.%s\" % sys.version_info[:2])')\n$ export PYTHONPATH=${HOME}/cctools/lib/python${PYVER}/site-packages:${PYTHONPATH}\n\nC Language Setup\n\nIf you are writing a TaskVine application in C, you should compile it into an executable with a command like this. Note that this example assumes that CCTools has been installed using the conda method.\n\ngcc taskvine_example.c -o taskvine_example -I${CONDA_PREFIX}/include/cctools -L${CONDA_PREFIX}/lib -ltaskvine -ldttools -lm -lz\n\nRunning a Manager Program\n\nThe example application simply compresses a bunch of files in parallel. The files to be compressed must be listed on the command line. Each will be transmitted to a remote worker, compressed, and then sent back to the manager. To compress files a, b, and c with this example application, run it as:\n\n# Python:\n$ ./taskvine_example.py a b c\n\n# C\n$ ./taskvine_example a b c\n\nYou will see this right away:\n\nlistening on port 9123...\nsubmitted task: /usr/bin/gzip < a > a.gz\nsubmitted task: /usr/bin/gzip < b > b.gz\nsubmitted task: /usr/bin/gzip < c > c.gz\nwaiting for tasks to complete...\n\nThe TaskVine manager is now waiting for workers to connect and begin requesting work. (Without any workers, it will wait forever.) You can start one worker on the same machine by opening a new shell and running:\n\n# Substitute the IP or name of your machine for MACHINENAME.\n$ vine_worker MACHINENAME 9123\n\nIf you have access to other machines, you can simply ssh there and run workers as well. In general, the more workers you start, the faster the work gets done. If a worker fails, the TaskVine infrastructure will retry the work elsewhere, so it is safe to submit many workers to an unreliable system.\nSubmitting Workers to a Batch System\n\nIf you have access to a HTCondor pool, you can use this shortcut to submit ten workers at once via HTCondor:\n\n$ vine_submit_workers -T condor MACHINENAME 9123 10\n\nSubmitting job(s)..........\nLogging submit event(s)..........\n10 job(s) submitted to cluster 298.\n\nThis will cause HTCondor to schedule worker jobs on remote machines. When they begin to run, they will call home to the indicated machine and port number, and begin to service the manager application.\n\nSimilar scripts are available for other common batch systems:\n\n$ vine_submit_workers -T slurm MACHINENAME 9123 10\n$ vine_submit_workers _T uge MACHINENAME 9123 10\n\nWhen the manager completes, if the workers were not otherwise shut down, they will still be available, so you can either run another manager with the same workers, or you can remove the workers with kill, condor_rm, or qdel as appropriate. If you forget to remove them, they will exit automatically after fifteen minutes. (This can be adjusted with the -t option to worker.)\nProject Names and the Catalog Server\n\nKeeping track of the manager's hostname and port can get cumbersome, especially if there are multiple managers. To help with this, a project name can be used to identify a TaskVine manager with a human-readable name. TaskVine workers can then be started for their managers by providing the project name instead of a host an port number.\n\nThe project name feature uses the Catalog Server to maintain and track the project names of managers and their respective locations. It works as follows: the manager advertises its project name along with its hostname and port to the catalog server. TaskVine workers that are provided with the manager's project name query the catalog server to find the hostname and port of the manager with the given project name.\n\nFor example, to have a TaskVine manager advertise its project name as myproject, add the following code snippet after creating the manager:\nPython\n\nm = vine.Manager(name = \"myproject\")\n\nC\n\nTo start a worker for this manager, specify the project name (myproject) to connect in the -M option:\n\n$ vine_worker -M myproject\n\nYou can start ten workers for this manager on Condor using vine_submit_workers by providing the same option arguments.:\n\n$ vine_submit_workers -T condor -M myproject 10\nSubmitting job(s)..........\nLogging submit event(s)..........\n10 job(s) submitted to cluster 298.\n\nOr similarly on UGE using vine_submit_workers as:\n\n$ vine_submit_workers -T uge -M myproject 10\nYour job 153097 (\"worker.sh\") has been submitted\nYour job 153098 (\"worker.sh\") has been submitted\nYour job 153099 (\"worker.sh\") has been submitted\n...\n\nTaskVine Online Status Display\n\nAn additional benefit of using a project name is that you can now use the vine_status command to display the progress of your application. This shows the name, location, and statistics of each application that reports itself to the catalog server. (Note that this information is updated about once per minute.). For example:\n\n% vine_status\nPROJECT               HOST                      PORT WAITING RUNNING COMPLETE WORKERS\nmolsim-c2h2           home.cse.nd.edu           8999     793      64      791      16\nfreds-model-search    mars.indiana.edu          9123     100     700     1372     350\nyang-analysis-355     login.crc.nd.edu          9100    8932    4873    10007    4873\n\nThe same information is available in a more graphical form online at the TaskVine Online Status, which looks like this:\n\nManaging Workers with the TaskVine Factory\n\nInstead of launching each worker manually from the command line, the utility vine_factory may be used to launch workers are needed. The factory will submit and maintain a number of workers according to the tasks available in one or more managers. For example, we can supply a minimum of 2 workers and a maximum of 10 to a manager with the project name myproject via the condor batch system as follows:\n\nvine_factory -Tcondor --min-workers=2 --max-workers=10 --manager-name myproject\n\nThis arguments can be specified in a file. The factory will periodically re-read this file, which allows adjustments to the number of workers desired:\n\nConfiguration file factory.json:\n\n{\n    \"manager-name\": \"myproject\",\n    \"max-workers\": 10,\n    \"min-workers\": 2\n}\n\nvine_factory -Tcondor -Cfactory.json\n\nFor further options, please refer to the TaskVine factory manual.\n\nBy default, the factory submits as many tasks that are waiting and running up to a specified maximum. To run more than one task in a worker, please refer to the following section on describing task resources and worker resources.\n\nWe can also create a factory directly in python. Creating a factory object does not immediately launch it, so this is a good time to configure the resources, number of workers, etc. Factory objects function as Python context managers, so to indicate that a set of commands should be run with a factory running, wrap them in a with statement. The factory will be cleaned up automtically at the end of the block. As an example:\n\nworkers = vine.Factory(\"condor\", \"myproject\")\nworkers.cores = 4\nworkers.memory = 4000\nworkers.disk = 5000\nworkers.max_workers = 20\nwith workers:\n    while not m.empty():\n        t = m.wait(5)\n        ...\n\nAdvanced Data Handling\nCaching and Sharing\n\nWherever possible, TaskVine retains files (whatever their sources) within the cluster so that they can be reused by later tasks. To do this consistently, each file is given a unique cache name that is computed from its contents and metadata. This ensures that if the external source for a file changes, any old cached copies will not be reused. In addition, cached files used concurrently by multiple tasks may be transferred between workers to share them efficiently.\n\nIf necessary, you can control the caching behavior of files individually.\n\n    A cache value of task indicates that the file should be deleted as soon as it is consumed by a task. This is appropriate for input files that are specific to one task, and one task only.\n    A cache value of workflow (the default) indicates that the file should be retained as long as the workflow runs, and then deleted at the end.\n    A cache value of worker indicates that the file should be retained by the worker until the worker's end-of-life.\n    A cache value of forever indicates that the file should be retained by the worker, even across workflows. This is appropriate for widely used software packages and reference datasets. This level of cache leaves files on the execution sites even when workers terminate, thus use with care.\n\nPython\n\nf = m.declare_file(\"myfile.txt\", cache=\"task\")       # (default, same as cache=False)\nf = m.declare_file(\"myfile.txt\", cache=\"workflow\")   # (same as cache=True)\nf = m.declare_file(\"myfile.txt\", cache=\"worker\")\nf = m.declare_file(\"myfile.txt\", cache=\"forever\")\n\nC\n\nTaskVine generally assumes that a file created on one worker can always be transferred to another. It is occasionally the case that a file created on a specific worker is truly specialized to that machine and should not be transferred. (For example, if a MiniTask compiled some code specifically for the architecture of a given machine.) In that case, you should indicate that peer transfers are not permitted:\nPython\n\nf = m.declare_file(\"myfile.txt\", cache=\"task\", peer_transfer=False)\n\nC\n\nAutomatic sharing of files between workers, or peer transfers, are enabled by default in TaskVine. If communication between workers is not possible or not desired, peer transfers may be globally disabled:\nPython\n\nm.disable_peer_transfers()\n\nC\n\nIf peer transfers have been disabled, they may be re-enabled accordingly:\nPython\n\nm.enable_peer_transfers()\n\nC\n\nTransfers between workers may be impacted by transient issues which may cause intermittent transfer failures. In these situations we take note of the failure that occured, and avoid using the same worker as a source for a period of time. This time period has a default value of 15 seconds. It may be changed by the user using vine_tune with the parameter transient-error-interval.\nMiniTasks\n\nA task can be used to perform custom fetch operations for input data. TaskVine calls these tasks mini tasks, and they are defined in the same way as regular tasks. Their only differences are that they are not submitted directly to the manager, and that their output (either a file or a directory) has to be specially declared.\n\nThis gives a lot of flexibility, as say for example, say you would like to expand a compressed file that TaskVine does not natively support, or you would like the input to be the result of a query to a database.\nPython\n\n# use cpio to expand archives coming from a url\nt = Task(\"cpio -iD output_dir < archive.cpio\")\n\nmy_url = m.declare_url(\"http://somewhere.com/archive.cpio\", cache=\"workflow\")\nt.add_input(my_url, \"archive.cpio\")\n\nmini_task = m.declare_mini_task(t, \"output_dir\")\n\n# regular tasks can use the mini task as input # the output of the mini\n# task is mounted in the regular task sandbox\n\nmy_other_task = Task(\"my_cmd output_from_cpio/\")\nmy_other_task.add_input(mini_task, \"output_from_cpio\")\n\n# we submit to the manager only the regular task\nm.submit(my_other_task)\n\nC\nExecution Contexts\n\nThe execution of a task can be wrapped with specially designed packages called execution contexts. These ensure that the software dependencies for the task are available in the execution site. TaskVine natively supports two types of environments: poncho, which is based on conda-pack; and starch, a lightweight package useful when the manager and workers run the same linux version. Mini tasks can be used to create environments not natively supported, as we will show later to construct execution contexts for Apptainer (i.e., singularity containers).\nPoncho\n\nA Poncho package is a tarball based on conda-pack, and is useful to deliver a complete python execution context. For example, to create a python package containing numpy:\n\nmy_poncho_spec.json\n\n{\n    \"conda\": {\n        \"channels\": [\n            \"conda-forge\"\n        ],\n        \"dependencies\": [\n            \"python=3.10\",\n            \"numpy=1.24.2\"\n        ]\n    }\n}\n\nFrom the command line, create the poncho package like this:\n\nponcho_package_create my_poncho_spec.json my_poncho_pkg.tar.gz\n\nAttach the package to the task:\nPython\n\n# my task that requires python and numpy\nt = Task(\"python my_numpy_script.py\")\n\ns = m.declare_file(\"my_numpy_script.py\", cache=\"workflow\")\nt.add_input(s, \"my_numpy_script.py\")\n\n# declare the package and its input file\nponcho_file = m.declare_file(\"my_poncho_pkg.tar.gz\", cache=\"workflow\")\nponcho_pkg = m.declare_poncho(poncho_file, cache=\"workflow\")\n\n# attach the package to the task\nt.add_poncho_package(poncho_pkg)\n\nm.submit(t)\n\nC\nStarch\n\n(to do)\nCustom Execution Contents\n\nTaskVine expects execution contents to expand to a directory, with this minimal structure:\n\nroot\n\u2514\u2500\u2500 bin\n    \u2514\u2500\u2500 run_in_env\n\nwhere run_in_env is an executable file (usually a shell script) that takes as an argument a command line to execute. In the rest of this section we will show how to construct an execution context that runs its command line inside an Apptainer container.\nApptainer Execution Context\n\nOur script run_in_env script simply calls Apptainer with the desired image, and mounts the task's sandbox as the home directory:\n\nrun_command_in_apptainer.sh\n\n#! /bin/sh\n\n# Wrap tasks with an Apptainer container\n\n# get the directory that contains the execution context from the location of this script\nctx_dir=$(dirname $( cd -- \"$( dirname -- \"$0\" )\" > /dev/null 2>&1 && pwd ))\n\n# execute the command line with the container image \"image.img\"\nexec apptainer exec --home \"${VINE_SANDBOX:-${PWD}}\" \"${ctx_dir}/image.sif\" \"$@\"\n\nTo start, we can manually construct in the command line the needed directory structure as follows. Later we will automate these steps with a mini task.\n\n# ensure the right execution permissions for the script\nchmod 755 run_command_in_apptainer.sh\n\n# construct the needed directory structure\nmkdir -p my_ctx/bin\n\n# copy the apptainer script to the expected run_in_env location\ncp run_command_in_apptainer.sh my_ctx/bin/run_in_env\n\n# copy the desired image into the package\ncp path/to/my_image.img my_ctx/image.img\n\nNow we are ready to declare the execution context from its local directory \"my_ctx\":\nPython\n\nt = Task(\"/bin/echo from inside apptainer!\")\n\nctx = m.declare_file(\"my_ctx\", cache=\"workflow\")\nt.add_execution_context(ctx)\n\nm.submit(t)\n\nC\nApptainer Execution Cpntext From a Mini Task\n\nIn the previous section we manually built the directory structure needed for the execution context. This is not very flexible, as we need to create one such directory per container image that we would like to use. Instead, we can use a mini task to construct the execution context directly on the workers.\nPython\n\n# construct the mini task. We only need the mini task for its sandbox to\n# create the environment structure, thus we use the command \":\" as no-op.\nmt = Task(\":\")\n\nrunner = m.declare_file(\"run_command_in_apptainer.sh\", cache=\"workflow\")\nimage  = m.declare_file(\"path/to/my_image.img\", cache=\"workflow\")\n\nmt.add_input(runner, \"ctx/bin/run_in_env\")\nmt.add_input(image,  \"ctx/image.img\")\n\n# the mini task will extract the environment directory\nctx = m.declare_mini_task(mt, \"ctx\")\n\n# now we define our regular task, and attach the environment to it.\nt = Task(\"/bin/echo from inside apptainer!\")\nt.add_execution_context(ctx)\n\nm.submit(t)\n\nYou can see the complete example here.\nWatching Output Files\n\nIf you would like to see the output of a task as it is produced, add the watch flag as an argument of add_file. This will cause the worker to periodically send output appended to that file back to the manager. This is useful for a program that produces a log or progress bar as part of its output.\nPython\n\nt.add_output_file(\"my-file\", watch=True)\n\nC\nOptional Output Files\n\nIt is sometimes useful to return an output file only in the case of a failed task. For example, if your task generates a very large debugging output file debug.out, then you might not want to keep the file if the task succeeded. In this case, you can mark the file as a \"failure-only\" output to indicate that it should only be returned when the task fails:\nPython\n\nmy_debug = m.declare_file(\"debug.out\", cache=\"task\")\nt.add_output(my_debug, \"debug.out\", failure_only=True)\n\nC\n\nIn a similar way, files can be marked to indicate that they should be returned on success:\nPython\n\nmy_debug = m.declare_file(\"debug.out\", cache=\"task\")\nt.add_output(my_debug, \"debug.out\", success_only=True)\n\nC\nAdvanced Task Handling\n\nA variety of advanced features are available for programs with unusual needs or very large scales. Each feature is described briefly here, and more details may be found in the TaskVine API.\nSecurity\n\nBy default, TaskVine does not perform any encryption or authentication, so any workers will be able to connect to your manager, and vice versa. This may be fine for a short running anonymous application, but is not safe for a long running application with a public name.\n\nWe recommend that, at a minimum, you enable an application password to provide authentication between managers and workers. And, consider enabling SSL to provide communication encryption. These features can be enabled independently.\nPassword Authentication\n\nWe recommend that you enable a password for your TaskVine applications. Create a file vine.password that contains a long string of random data like this:\n\nopenssl rand -hex 32 > vine.password\n\nThis password will be particular to your application, and only managers and workers with the same password will be able to interoperator. Then, modify your manager program to use the password:\nPython\n\nm.set_password_file(\"vine.password\")\n\nC\n\nAnd give the --password option to give the same password file to your workers:\n\n$ vine_worker --password vine.password -M myproject\n\nWith this option enabled, both the manager and the workers will verify that the other has the matching password before proceeding. Likewise, when workers perform peer-to-peer transfers, the password will be verified.\n\nNote that the password is not sent in the clear, but is securely verified through a SHA1-based mutual challenge-response protocol.\nSSL Encryption\n\nTaskVine can encrypt the communication between manager and workers using SSL. For this, you need to set the key and certificate (in PEM format) of your server when creating the manager.\n\nIf you do not have a key and certificate at hand, but you want the communications to be encrypted, you can create your own key and certificate:\n\n# Be aware that since this certificate would not be signed by any authority, it\n# cannot be used to prove the identity of the server running the manager.\n\nopenssl req -x509 -newkey rsa:4096 -keyout MY_KEY.pem -out MY_CERT.pem -sha256 -days 365 -nodes\n\nTo activate SSL encryption, indicate the paths to the key and certificate when creating the manager:\nPython\n\n# Import the taskvine module\nimport ndcctools.taskvine as vine\nm = vine.Manager(port=9123, ssl=('MY_KEY.pem', 'MY_CERT.pem'))\n\n# Alternatively, you can set ssl=True and let the python API generate\n# temporary ssl credentials for the manager:\nm = vine.Manager(port=9123, ssl=True)\n\nC\n\nIf you are using a project name for your manager, then the workers will be aware that the manager is using SSL and communicate accordingly automatically. However, you are directly specifying the address of the manager when launching the workers, then you need to add the --ssl flag to the command line, as:\n\nvine_worker (... other args ...) --ssl HOST PORT\nvine_factory (... other args ...) --ssl HOST PORT\nvine_status --ssl HOST PORT\nvine_submit_workers -T condor -E'--ssl' HOST PORT\n\nMaximum Retries\n\nWhen a task cannot be completed because a worker disconnects or because it exhausted some intermediate resource allocation, it is automatically retried. By default, there is no limit on the number of retries. However, you can set a limit on the number of retries:\nPython\n\nt.set_retries(5)   # Task will be try at most 6 times (5 retries).\n\n# this can be done at task declaration as well:\n t = vine.Task(\n    command = ...,\n    retries = 5\n )\n\nC\n\nWhen a task cannot be completed in the set number of tries, then the its result is set to the result of the last attempt (e.g. \"resource exhaustion\" in python, or VINE_RESULT_RESOURCE_EXHAUSTION in C).\nPipelined Submission\n\nIf you have a very large number of tasks to run, it may not be possible to submit all of the tasks, and then wait for all of them. Instead, submit a small number of tasks, then alternate waiting and submitting to keep a constant number in the manager. The hungry will tell you if more submissions are warranted:\nPython\n\nif m.hungry():\n    # submit more tasks...\n\nC\nAutomatic Garbage Collection on Disk\n\nFor workflows that generate partial results that are not needed once a final result has been computed, TaskVine can automatically delete them from disk when the application indicates that they will not be needed anymore:\nPython\n\npartial_result = m.declare_file(\"my_partial_result\", unlink_when_done=True)\n\nt1 = Task(...)\nt1.add_output(partial_result, \"my_partial_result\")\n...\n\nt2 = Task(...)\nt2.add_input(partial_result, \"my_partial_result\")\n...\n\n# once t2 is done, the following call will remove the file from the\n# taskvine workflow. Further, when no task refers to the file, the file\n# will be removed from the manager's disk because of unlink_when_done=True\n# at its declaration.\nm.undeclare_file(partial_result)\n\nC\n\nWarning\n\nNever use this feature on files that the TaskVine application did not create. Otherwise you run the risk of removing irreplaceable input files\nDisconnect slow workers\n\nA large computation can often be slowed down by stragglers. If you have a large number of small tasks that take a short amount of time, then automatically disconnecting slow workers can help. With this feature enabled, statistics are kept on tasks execution times and statistical outliers are terminated. If two different tasks are canceled in the same worker, then the worker is disconnected and blacklisted.\nPython\n\n# Disconnect workers that are executing tasks twice as slow as compared to the average.\nm.enable_disconnect_slow_workers(2)\n\nC\n\nTasks terminated this way are automatically retried in some other worker. Each retry allows the task to run for longer times until a completion is reached. You can set an upper bound in the number of retries with Maximum Retries.\nString Interpolation\n\nIf you have workers distributed across multiple operating systems (such as Linux, Cygwin, Solaris) and/or architectures (such as i686, x86_64) and have files specific to each of these systems, this feature will help. The strings $OS and $ARCH are available for use in the specification of input file names. TaskVine will automatically resolve these strings to the operating system and architecture of each connected worker and transfer the input file corresponding to the resolved file name. For example:\nPython\n\nmy_exec = m.declare_file(\"my-executable.$OS.$ARCH\",  cache=\"workflow\")\nt.add_input_input(my_exec, \"my_exe\")\n\nC\n\nThis will transfer my-executable.Linux.x86_64 to workers running on a Linux system with an x86_64 architecture and a.Cygwin.i686 to workers on Cygwin with an i686 architecture. These files will be named \"my_exe\" in the task's sandbox, which means that the command line of the tasks does not need to change.\n\nNote this feature is specifically designed for specifying and distingushing input file names for different platforms and architectures. Also, this is different from the $VINE_SANDBOX shell environment variable that exports the location of the working directory of the worker to its execution environment.\nTask Cancellations\n\nThis feature is useful in workflows where there are redundant tasks or tasks that become obsolete as other tasks finish. Tasks can be removed either by either task_id or tag. Tasks removed in this way will still be returned in the usual way via wait with a result of VINE_RESULT_CANCELLED. For example:\nPython\n\n# create task as usual and tag it with an arbitrary string.\nt = vine.Task(...)\nt.set_tag(\"my-tag\")\n\n# or set tag in task declaration\nt = vine.Task(\n    command = ...,\n    tag = \"my-tag\"\n)\n\ntaskid = m.submit(t)\n\n# cancel task by id.\nm.cancel_by_taskid(taskid)\n\n# or cancel task by tag.\nm.cancel_by_tasktag(\"my-tag\")\n\nC\n\nNote\n\nIf several tasks have the same tag, only one of them is cancelled. If you want to cancel all the tasks with the same tag, you can use loop until cancel_by_task returns zero:\n\n    while m.cancel_by_taskid(\"my-tag\")>0:\n        pass\n\nBlocking workers\n\nYou may find that certain hosts are not correctly configured to run your tasks. The manager can be directed to ignore certain workers, as:\nPython\n\nt = m.wait(5)\n\n# if t fails given a worker misconfiguration:\nm.block_host(t.hostname)\n\nC\nPerformance Statistics\n\nThe manager tracks a fair number of statistics that count the number of tasks, number of workers, number of failures, and so forth. This information is useful to make a progress bar or other user-visible information:\nPython\n\nstats = m.stats\nprint(stats.workers_busy)\n\nC\nPython Programming Models\n\nWhen writing a manager in Python, you have access to several types of tasks that wrap around the standard task abstraction:\nPython Tasks\n\nA PythonTask is an extension of a standard task. It is not defined with a command line to execute, but with a Python function and its arguments, like this:\nPython\n\ndef my_sum(x, y):\n    return x+y\n\n# task to execute x = my_sum(1, 2)\nt = vine.PythonTask(my_sum, 1, 2)\n\nA PythonTask is handled in the same way as a standard task, except that its output t.output is simply the Python return value of the function. If the function should throw an exception, then the output will be the exception object.\n\nYou can examine the result of a PythonTask like this:\nPython\n\nwhile not m.empty():\n    t = m.wait(5)\n    if t:\n        x = t.output\n        if isinstance(x, Exception):\n            print(\"Exception: {}\".format(x))\n        else:\n            print(\"Result: {}\".format(x))\n\nA PythonTask is derived from Task and so all other methods for controlling scheduling, managing resources, and setting performance options all apply to PythonTask as well.\n\nWhen running a Python function remotely, it is assumed that the Python interpreter and libraries available at the worker correspond to the appropiate python environment for the task. If this is not the case, an environment file can be provided with t.set_environment:\nPython\n\nt = vine.PythonTask(my_sum, 1, 2)\nt.set_environment(\"my-env.tar.gz\")\n\nThe file my-env.tar.gz is a conda environment created with conda-pack. A minimal environment can be created a follows:\n\nconda create -y -p my-env python=3.8 cloudpickle conda\nconda install -y -p my-env -c conda-forge conda-pack\n# conda install -y -p my-env pip and conda install other modules, etc.\nconda run -p my-env conda-pack\n\nServerless Computing\n\nTaskVine offers a serverless computing model which is especially appropriate for invoking functions that have a relatively short execution time (10s or less) and have substantial startup time due to large numbers of libraries or dependent data.\n\nIn this model, you first define and install a LibraryTask that defines a function, and then invoke FunctionCall tasks that invoke the library by name.\n\nSuppose your main program has two functions my_sum and my_mul. Invoke create_library_from_functions to package up these function definitions into a library task libtask\nPython\n\ndef my_sum(x, y):\n    return x+y\n\ndef my_mul(x, y):\n    return x*y\n\nlibtask = m.create_library_from_functions(\"my_library\", my_sum, my_mul)\n\nWe strongly recommend to specify the modules the function needs inside the function itself. This ensures that the correct modules and their aliases will be available when the functions are executed in isolation at the worker:\n\nYou can certainly embed import statements within the function and install any necessary packages:\nPython\n\ndef divide(dividend, divisor): \n    import math \n    return dividend / math.sqrt(divisor)\n\nlibtask = m.create_library_from_functions(\"my_library\", divide)\n\nIf the overhead of importing modules per function is noticeable, modules can be optionally imported as a common preamble to the function executions. Common modules can be specified with the hoisting_modules argument to create_library_from_functions. This reduces the overhead by eliminating redundant imports:\nPython\n\nimport numpy\nimport math\n\nhoisting_modules = [numpy, math]\n\nhoisting_modules only accepts modules as arguments (e.g. it can't be used to import functions, or select particular names with from ... import ... statements. Such statements should be made inside functions after specifying the modules with hoisting_modules.\nPython\n\ndef cube(x):\n    # whenever using FromImport statments, put them inside of functions\n    from random import uniform\n    from time import sleep as time_sleep\n\n    random_delay = uniform(0.00001, 0.0001)\n    time_sleep(random_delay)\n\n    return math.pow(x, 3)\n\nAfter installing the packages and functions, you can optionally specify the number of functions the library can run concurrently by setting the number of function slots. (If unset, TaskVine will assume the library can run one function per core available.)\nPython\n\nlibtask.set_function_slots(4)   # maximum 4 concurrent functions\n\nOnce complete, the library task must be installed in the system:\nPython\n\nm.install_library(t)\n\nThis causes the library task to be dispatched and started at available workers, where it remains running. Immediately after installing the library, you may submit FunctionCall tasks that invoke the library and functions by name:\nPython\n\nt = vine.FunctionCall(\"my_library\", \"my_mul\", 20, 30);\nt.set_cores(1)\nt.set_memory(100)\nt.set_disk(100)\nm.submit(t)\n\nThe function invocation will be dispatched to available workers, and when it is returned, the result is present as t.output:\nPython\n\nt = m.wait(5)\nif t:\n    print(t.output)\n\nNote that both library tasks and function invocations consume resources at the worker, and the number of running tasks will be constrained by the available resources in the same way as normal tasks.\nStateful Serverless Computing\n\nA function typically sets up its states (e.g., load modules/packages, build internal models or states) before executing its computation. With advanced serverless computing in TaskVine, you can set up a shared state between function invocations so the cost of setting up states doesn't have to be paid for every invocation, but instead is paid once and shared many times. TaskVine supports this technique as demonstrated via the below example.\n\nAssume that you program has two functions my_sum and my_mul, and they both use base to set up a common value in their computations.\nPython\n\ndef base(x, y=1):\n    return x**y\n\nA = 2\nB = 3\n\ndef my_sum(x, y):\n    base_val = base(A, B)\n    return base_val + x+y\n\ndef my_mul(x, y):\n    base_val = base(A, B)\n    return base_val + x*y\n\nWith this setup, base(A, B) has to be called repeatedly for every function invocation of my_sum and my_mul. What you want instead is to have the value of base(A, B) created and computed once and stored in a library. my_sum and my_mul thus only have to load such value, instead of computing the value, from a library's state, as follows.\nPython\n\nfrom ndcctools.taskvine.utils import load_variable_from_library\ndef base(x, y=1):\n    return {'base_val': x**y}\n\nA = 2\nB = 3\n\ndef my_sum(x, y):\n    base_val = load_variable_from_library('base_val')\n    return base_val + x+y\n\ndef my_mul(x, y):\n    base_val = load_variable_from_library('base_val')\n    return base_val + x*y\n\nlibtask = m.create_library_from_functions(\"my_library\", my_sum, my_mul, library_context_info=[base, [A], {'y': B})\nm.install(libtask)\n# application continues as usual with submitting FunctionCalls and waiting for results.\n...\n\nThis technique enables maximum sharing between invocations of functions that share some common states, and between invocations of the same function in a library. This is especially helpful in ML/AI workloads where one has to build an ML/AI model on a remote node to best configure it against the remote node's local resources (e.g., GPU). Thus, instead of loading and creating a model for every invocation:\nPython\n\ndef infer(image):\n    # load model parameters\n    ...\n    # build model\n    model = tf.ResNet50(...)\n    # load model in GPU\n    model.to_gpu(1)\n    # execute an inference\n    return model.infer(image)\n\nOne can do this to have the model created and loaded in a GPU once and separate the model creation from the actual inference:\nPython\n\nfrom ndcctools.taskvine.utils import load_variable_from_library\ndef model_setup():\n    # load model parameters\n    ...\n    # build model\n    model = tf.ResNet50(...)\n    # load model in GPU\n    model.to_gpu(1)\n    return {'model': model}\n\ndef infer(image):\n    model = load_variable_from_library('model')\n    # execute an inference\n    return model.infer(image)\n\nlibtask = m.create_library_from_functions('infer_library',\n                                          infer,\n                                          library_context_info=[model_setup, [], {})\nm.install(libtask)\n\n# application continues as usual with submitting FunctionCalls and waiting for results.\n...\n\nFutures\n\nTaskVine provides a futures executor model which is a subclass of Python's concurrent futures executor. A function along with its arguments are submitted to the executor to be executed. A future is returned whose value will be resolved at some later point.\n\nTo create a future, a FuturesExecutor object must first be created. Tasks can then be submitted through the submit function. This will return a Future object. The result of the task can retrieved by calling future.result()\nPython\n\nimport ndcctools.taskvine as vine\n\ndef my_sum(x, y):\n    return x + y\n\nm = vine.FuturesExecutor(manager_name='my_manager')\n\na = m.submit(my_sum, 3, 4)\nb = m.submit(my_sum, 5, 2)\nc = m.submit(my_sum, a, b)  # note that the futures a and b are\n                            # passed as any other argument.\n\nprint(c.result())\n\nIf the tasks need to be configured in some way, for example to specify maximum resources allowed, the method future_task returns a FuturePythonTask that can be tailored as any other task:\nPython\n\nimport ndcctools.taskvine as vine\n\ndef my_sum(x, y):\n    return x + y\n\nm = vine.FuturesExecutor(manager_name='my_manager')\n\nt = m.future_task(my_sum, 3, 4)\nt.set_cores(1)\n\nf = m.submit(t)\n\nprint(f.result())\n\nAdditionally, the executor the Vine Factory to submit TaskVine workers. Specifications for the workers can be provided via the opts keyword argument when creating to executor.\nPython\n\n```python import ndcctools.taskvine as vine\n\ndef my_sum(x, y): return x + y\n\nopts = {\"memory\": 8000, \"disk\":8000, \"cores\":8, \"min-workers\": 5} m = vine.FuturesExecutor(manager_name='my_manager', batch_type=\"condor\", opts=opts)\n\nt = m.future_task(my_sum, 3, 4) t.set_cores(1)\n\nf = m.submit(t)\n\nprint(f.result())\n\nInstead of tasks, the futures may also executed using function calls with the future_funcall method:\nPython\n\nimport ndcctools.taskvine as vine\n\ndef my_sum(x, y):\n    return x + y\n\nm = vine.FuturesExecutor(manager_name='my_manager')\n\nlibtask = m.create_library_from_functions('test-library', my_sum)\nm.install_library(libtask)\n\nt = m.future_funcall('test-library', 'my_sum', 7, 4)\n\na = m.submit(t)\n\nprint(a.result())\n\nFunctional Abstractions\n\nThe TaskVine *map abstraction works similar to python map, as it applies a a function to every element in a list. This function works by taking in a chunk_size, which is the size of an iterable to send to a worker. The worker than maps the given function over the iterable and returns it. All the results are then combined from the workers and returned. The size of the chunk depends on the cost of the function. If the function is very cheap, then sending a larger chunk_size is better. If the function is expensive, then smaller is better. If an invalid operation happens, the error will appear in the results.\n\ndef fn(a):\n    return a*a\n\nm.map(fn, arry, chunk_size)\n\nThe TaskVine pair function computes all the pairs of 2 sequences, and then uses them as inputs of a given function. The pairs are generated locally using itertools, and then based on the given chunk_size, are sent out to a worker as an iterable of pairs. The given function must accept an iterable, as the pair will be sent to the function as a tuple. The worker will then return the results, and each result from each worker will be combined locally. Again, cheaper functions work better with larger chunk_sizes, more expensive functions work better with smaller ones. Errors will be placed in results.\n\ndef fn(pair):\n    return pair[0] * pair[1]\n\nm.pair(fn, seq1, seq2, chunk_size)\n\nThe treeReduce function combines an array using a given function by breaking up the array into chunk_sized chunks, computing the results, and returning the results to a new array. It then does the same process on the new array until there only one element left and then returns it. The given fucntion must accept an iterable, and must be an associative fucntion, or else the same result cannot be gaurenteed for different chunk sizes. Again, cheaper functions work better with larger chunk_sizes, more expensive functions work better with smaller ones. Errors will be placed in results. Also, the minimum chunk size is 2, as going 1 element at time would not reduce the array\n\ndef fn(seq):\n    return max(seq)\n\nm.treeReduce(fn, arry, chunk_size)\n\nBelow is an example of all three abstractions, and their expected output:\n\n# abstractions.py\n\nimport ndcctools.taskvine as vine\n\ndef main():\n    # Set up queue\n    q = vine.Manager(port=9123)\n\n    # map - similar to Python's own map function, but uses a taskvine worker\n    # to complete computation. Returns sequence with the results from the given function\n    # [result] = q.map(func, sequence)\n    # Example: (returns [1, 4, 9, 16])\n    results = q.map(lambda x: x*x, [1, 2, 3, 4])\n    print(results)\n\n    # pair - similar to map function, but uses the function for every pair between\n    # the two sequences. Returns sequence of results of each pair.\n    # [result] = q.pair(func, sequence1, sequence2)\n    # Example: (returns [1, 2, 3, 4, 2, 4, 6, 8, 3, 6, 9, 12, 4, 8, 12, 16])\n    results = q.pair(lambda x, y: x*y, [1, 2, 3, 4], [1, 2, 3, 4])\n    print(results)\n\n    # tree_reduce - combines pairs of values using a given function, and then returns\n    # to a single final number after reducing the sequence.\n    # result = q.tree_reduce(func, sequence)\n    # Example (even): (returns 24)\n    results = q.tree_reduce(lambda x, y: x*y, [1, 2, 3, 4])\n    print(results)\n\n    # Example (odd): (returns 120)\n    results = q.tree_reduce(lambda x, y: x*y, [1, 2, 3, 4, 5])\n    print(results)\n\n\nif __name__ == \"__main__\":\n    main()\n\nRun:\n\npython abstractions.py\n\nExpected output:\n\nMap: [2, 4, 6, 8]\nPair: [2, 4, 6, 8, 4, 8, 12, 16, 6, 12, 18, 24, 8, 18, 24, 32]\nTree: 8\n\nManaging Resources\n\nUnless otherwise specified, TaskVine assumes that a single task runs on a single worker at a time, and a single worker occupies an entire machine.\n\nHowever, if the resources at a machine are larger than what you know a task requires, you most likely will want one worker to manage multiple tasks running on that machine. For example, if you have a 8-core machine, then you might want to run four 2-core tasks on a single worker at once, being careful not to exceed the available memory and disk.\nTask Resources\n\nTo run several tasks in a worker, every task must have a description of the resources it uses, in terms of cores, memory, disk, and gpus. While time is not exactly a type of resource, specifying the running time of tasks can often be helpful to map tasks to workers. These resources can be specified as in the following example:\nPython\n\nt.set_cores(1)           # task needs one core\nt.set_memory(1024)       # task needs 1024 MB of memory\nt.set_disk(4096)         # task needs 4096 MB of disk space\nt.set_gpus(0)            # task does not need a gpu\nt.set_time_max(100)      # task is allowed to run in 100 seconds\nt.set_time_min(10)       # task needs at least 10 seconds to run (see vine_worker --wall-time option above)\nt.add_feature(\"NVIDIA RTX A2000\")  # task requires this specific GPU type\n\n# these can be set when the task is declared as well:\n t = vine.Task(\n    command = \"./gzip < my-file > my-file.gz\",\n    cores = 1,\n    memory = 1024,\n    disk = 4096,\n    gpus = 0,\n    time_max = 100,\n    time_min = 10\n)\n\nC\n\nWhen the maximum running time is specified, TaskVine will kill any task that exceeds its maximum running time. The minimum running time, if specified, helps TaskVine decide which worker best fits which task. Specifying tasks' running time is especially helpful in clusters where workers may have a hard threshold of their running time.\n\nResources are allocated according to the following rules:\n\n    If the task does not specify any resources, then it is allocated a whole worker.\n    The task will be allocated as least as much of the value of the resources specified. E.g., a task that specifies two cores will be allocated at least two cores.\n    If gpus remain unspecified, then the task is allocated zero gpus.\n    If a task specifies gpus, but does not specify cores, then the task is allocated zero cores.\n    In all other cases, cores, memory, and disk of the worker are divided evenly according to the maximum proportion of specified task requirements over worker resources. The proportions are rounded up so that only whole number of tasks could fit in the worker.\n\nAs an example, consider a task that only specifies 1 core, and does not specify any other resource, and a worker with 4 cores, 12 GB of memory, and 36 GB of disk. According to the rules above:\n\n    Rule 1 does not apply, as at least one resource (cores) was specified.\n    According to rule 2, the task will get at least one core.\n    According to rule 3, the task will not be allocated any gpus.\n    Rule 4 does not apply, as no gpus were specified, and cores were specified.\n    For rule 5, the task requires 1 core, and the worker has 4 cores. This gives a proportion of 1/4=0.25. Thus, the task is assigned 25% of the memory and disk (3 GB and 9 GB respectively).\n\nAs another example, now assume that the task specifies 1 cores and 6 GB of memory:\n\n    Rules 1 to 4 are as the last example, only that now the task will get at least 6 GB of memory.\n    From cores we get a proportion of 1/4=0.25, and from memory 6GB/12GB=0.5. The memory proportion dictates the allocation as it is the largest. This means that the task will get assigned 50% of the cores (2), memory (6 GB), and disk (18 GB).\n\nNote that proportions are 'rounded up', as the following example shows. Consider now that the task requires 1 cores, 6GB of memory, and 27 GB of disk:\n\n    Rules 1 to 4 are as before, only that now the worker will get at least 30 GB of disk.\n    The proportions are 1/4=0.25 for cores, 6GB/12GB=0.5 for memory, and 27GB/36GB=0.75 for disk. This would assign 3 cores, 9 memory, and 27 to the task. However, this would mean that no other task of this size would be able to run in the worker. Rather than assign 75% of the resources and risk an preventable failure because of resource exhaustion, the task is assigned 100% of the resources from the worker. More generally, allocations are rounded up so that only a whole number of tasks can be fit in the worker.\n\nNote\n\nIf you want TaskVine to exactly allocate the resources you have specified, use m.disable_proportional_resources() (see also proportional-whole-tasks here. In general, however, we have found that using proportions nicely adapts to the underlying available resources, and leads to very few resource exhaustion failures while still using worker resources efficiently.\n\nThe current TaskVine implementation only accepts whole integers for its resources, which means that no worker can concurrently execute more tasks than its number of cores. (This will likely change in the future.)\n\nWhen you would like to run several tasks in a worker, but you are not sure about the resources each task needs, TaskVine can automatically find values of resources that maximize throughput, or minimize waste. This is discussed in the section below.\nWorker Resources\n\nBy default, a worker tries to use all the resources of the machine it is running. The resources detected are displayed when the worker starts up, for example:\n\nvine_worker: creating workspace /tmp/worker-102744-8066\nvine_worker: using 16 cores, 15843 MB memory, 61291 MB disk, 0 gpus\n\nYou can manually adjust the resources managed by a worker like this:\n\n$ vine_worker --cores 8  --memory 1000 --disk 8000 --gpus 1 ...other options...\n\nUnlike other resources, the default value for gpus is 0. You can use the command line option --gpus to declare how many gpus are available at a worker.\n\nWhen the lifetime of the worker is known, for example, the end of life of a lease, this information can be communicated to the worker as follows. For example, if the worker will be terminated in one hour:\n\n$ vine_worker --wall-time 3600 ...other options...\n\nIn combination with the worker option --wall-time, tasks can request a minimum time to execute with set_time_min, as explained (below)[#setting-task-resources].\n\nYou may also use the same --cores, --memory, --disk, and --gpus options when using batch submission script vine_submit_workers, and the script will correctly ask the right batch system for a node of the desired size.\n\nThe only caveat is when using vine_submit_workers -T uge, as there are many differences across systems that the script cannot manage. For vine_submit_workers -T uge you have to set both the resources used by the worker (i.e., with --cores, etc.) and the appropiate computing node with the -p option.\n\nFor example, say that your local UGE installation requires you to set the number of cores with the switch -pe smp , and you want workers with 4 cores:\n\n$ vine_submit_workers -T uge --cores 4 -p \"-pe smp 4\" MACHINENAME 9123\n\nIf you find that there are options that are needed everytime, you can compile CCTools using the --uge-parameter. For example, at Notre Dame we automatically set the number of cores as follows:\n\n$ ./configure  --uge-parameter '-pe smp $cores'\n\nSo that we can simply call:\n\n$ vine_submit_workers -T uge --cores 4 MACHINENAME 9123\n\nThe variables $cores, $memory, and $disk, have the values of the options passed to --cores, --memory, --disk.\nFactory Resources\n\nThe vine_factory accepts the arguments --cores, --memory, --disk, and --gpus to set the size of the desired workers. Resources may also be set in the configuration file as follows:\n\n{\n    \"manager-name\": \"myproject\",\n    \"max-workers\": 4,\n    \"min-workers\": 1,\n    \"cores\": 4,\n    \"memory\": 4096,\n    \"disk\": 4096,\n    \"gpus\": 1\n}\n\nGPU Types and Custom Features\n\nIt is sometimes necessary to match a task to a worker that has a specific capability. Perhaps your pool of workers has two different kinds of GPUs. The type of a GPU is automatically reported as a \"feature\" that tasks can select.\n\nTo describe a task that can only run on a specific GPU type, use add_feature:\nPython\n\nt.add_feature(\"NVIDIA RTX A2000\") # task requires worker with this feature\n\nC\n\n(Note that the GPU feature is automatically reported by the worker when it starts up.)\n\nvine_worker: using 4 cores, 15610 MB memory, 33859 MB disk, 1 gpus\nvine_worker: gpu is called feature \"NVIDIA RTX A2000\"\n\nYou may also add additional custom features to a worker at startup time using the --feature option:\n\nvine_worker ... --feature alpha --feature beta ...\n\nOr, use the factory to start a large number of workers with that feature:\n\nvine_factory ... --feature alpha --feature beta ...\n\nMonitoring and Enforcement\n\nSo far we have used resources values simply as hints to TaskVine to schedule concurrent tasks at workers. By default, TaskVine does not monitor or enforce these limits. You can enable monitoring and enforcement as follows:\nPython\n\n# Measure the resources used by tasks, and terminate tasks that go above their\n# resources:\nm.enable_monitoring()\n\n# Measure the resources used by tasks, but do not terminate tasks that go above\n# declared resources:\nm.enable_monitoring(watchdog=False)\n\n# Measure the resources used by tasks, but do not terminate tasks that go\n# above declared resources, and generate a time series per task. These time\n# series are written to the logs directory `vine-logs/time-series`.\n# Use with caution, as time series for long running tasks may be in the\n# order of gigabytes. \nm.enable_monitoring(m, watchdog=False, time_series=True)\n\nC\n\nWhen monitoring is enabled, you can explore the resources measured when a task returns:\nPython\n\nt = m.wait(5)\nif t:\n    print(\"Task used {} cores, {} MB memory, {} MB disk\",\n        t.resources_measured.cores,\n        t.resources_measured.memory,\n        t.resources_measured.disk)\n    print(\"Task was allocated {} cores, {} MB memory, {} MB disk\",\n        t.resources_requested.cores,\n        t.resources_requested.memory,\n        t.resources_requested.disk)\n    if t.limits_exceeded and t.limits_exceeded.cores > -1:\n        print(\"Task exceeded its cores allocation.\")\n\nC\n\nAlternatively, when you declare a task (i.e., before submitting it), you can declare a directory to which a report of the resources will be written. The report format is JSON, as its filename has the form vine-PID_OF_MANAGER-task-TASK_ID.summary.\nPython\n\nt = vine.Task(...)\nt.set_monitor_output(\"my-resources-output\")\n...\ntaskid = m.submit(t)\n\n# this can be set at declaration as well:\n t = vine.Task(\n    command = ...,\n    monitor_output = \"my-resources-output\"\n )\n\nC\n\nTaskVine also measures other resources, such as peak bandwidth, bytes_read, bytes_written, bytes_sent, bytes_received, total_files, cpu_time, and wall_time.\nGrouping Tasks with Similar Resource Needs\n\nSeveral tasks usually share the same resource description, and to this end, TaskVine allows you to tasks into groups called categories. You can attach resource descriptions to each category, and then label a task to set it as part of a category.\n\nWe can create some categories with their resource description as follows:\nPython\n\n# memory and disk values in MB.\nm.set_category_resources_max('my-category-a', {'cores': 2, 'memory': 1024, 'disk': 2048, 'gpus': 0})\nm.set_category_resources_max('my-category-b', {'cores': 1})\nm.set_category_resources_max('my-category-c', {})\n\nC\n\nIn the previous examples, we created three categories. Note that it is not necessary to set all the resources, as TaskVine can be directed to compute some efficient defaults. To assign a task to a category:\nPython\n\nt.set_category('my-category-a')\n\n# alternatively:\n t = vine.Task(\n    command = ...,\n    category = 'my-category-a'\n )\n\nC\n\nWhen a category leaves some resource unspecified, then TaskVine tries to find some reasonable defaults in the same way described before in the section (Specifying Task Resources)[#setting-task-resources].\n\nWarning\n\nWhen a task is declared as part of a category, and also has resources set directly with calls such as t.set_cores, the resources directly set take precedence over the category declaration for that task\n\nWhen the resources used by a task are unknown, TaskVine can measure and compute efficient resource values to maximize throughput or minimize waste, as we explain in the following sections.\nAutomatic Resource Management\n\nIf the resources a category uses are unknown, then TaskVine can be directed to find efficient resource values to maximize throughput or minimize resources wasted. In these modes, if a value for a resource is set with set_resources_max, then it is used as a theoretical maximum.\n\nWhen automatically computing resources, if any of cores, memory or disk are left unspecified in set_resources_max, then TaskVine will run some tasks using whole workers to collect some resource usage statistics. If all cores, memory, and disk are set, then TaskVine uses these maximum values instead of using whole workers. As before, unspecified gpus default to 0.\n\nOnce some statistics are available, further tasks may run with smaller allocations if such a change would increase throughput. Should a task exhaust its resources, it will be retried using the values of set_resources_max, or a whole worker, as explained before.\n\nAutomatic resource management is enabled per category as follows:\nPython\n\nm.enable_monitoring()\nm.set_category_resources_max('my-category-a', {})\nm.set_category_mode('my-category-a', \"max throughput\")\n\nm.set_category_resources_max('my-category-b', {'cores': 2})\nm.set_category_mode('my-category-b', \"max throughput\")\n\nC\n\nIn the previous examples, tasks in 'my-category-b' will never use more than two cores, while tasks in 'my-category-a' are free to use as many cores as the largest worker available if needed.\n\nYou can set a limit on the minimum resource value a category can use. The automatic resource computation will never go below the values set:\nPython\n\nm.set_category_resources_min('my-category-a', {'memory': 512})\n\nC\n\nYou can enquire about the resources computed per category with vine_status:\n\n$ vine_status -A  IP-OF-MACHINE-HOSTING-WQ PORT-OF-WQ\nCATEGORY        RUNNING    WAITING  FIT-WORKERS  MAX-CORES MAX-MEMORY   MAX-DISK\nanalysis            216        784           54          4      ~1011      ~3502\nmerge                20         92           30         ~1      ~4021      21318\ndefault               1         25           54         >1       ~503       >243\n\nIn the above, we have three categories, with RUNNING and WAITING tasks. The column FIT-WORKERS shows the count of workers that can fit at least one task in that category using the maximum resources either set or found. Values for max cores, memory and disk have modifiers ~ and > as follows:\n\n    No modifier: The maximum resource usage set with set_category_resources_max, or set for any task in the category via calls such as set_cores.\n    ~: The maximum resource usage so far seen when resource is left unspecified in set_category_resources_max. All tasks so far have run with no more than this resource value allocated.\n\n        : The maximum resource usage that has caused a resource exhaustion. If this value is larger than then one set with set_category_resources_max, then tasks that exhaust resources are not retried. Otherwise, if a maximum was not set, the tasks will be retried in larger workers as workers become available.\n\nWarning\n\nWhen resources are set directly to the task with calls such as t.set_cores, such resources are fixed for the task and are not modified when more efficient values are found.\n", "The detailed API description of the ndcctools.taskvine library is below, including all classes, methods, and parameters with descriptions:\nndcctools.taskvine.manager.Manager\n  __init__(self, port, name, shutdown, run_info_path, staging_path, ssl, init_fn, status_display_interval) - Create a new manager.\n    self - Reference to the current manager object.\n    port - The port number to listen on. If zero, then a random port is chosen. A range of possible ports (low, hight) can be also specified instead of a single integer. Default is 9123\n    name - The project name to use.\n    shutdown - Automatically shutdown workers when manager is finished. Disabled by default.\n    run_info_path - Directory to write log (and staging if staging_path not given) files per run. If None, defaults to \"vine-run-info\"\n    staging_path - Directory to write temporary files. Defaults to run_info_path if not given.\n    ssl - A tuple of filenames (ssl_key, ssl_cert) in pem format, or True. If not given, then TSL is not activated. If True, a self-signed temporary key and cert are generated.\n    init_fn - Function applied to the newly created manager at initialization.\n    status_display_interval - Number of seconds between updates to the jupyter status display. None, or less than 1 disables it.\n  name() - Get the project name of the manager.\n  port() - Get the listening port of the manager.\n  using_ssl() - Whether the manager is using ssl to talk to workers.\n  logging_directory() - Get the logs directory of the manager.\n  staging_directory() - Get the staging directory of the manager.\n  library_logging_directory() - Get the library logs directory of the manager.\n  cache_directory() - Get the caching directory of the manager.\n  stats() - Get manager statistics.\n  stats_category(self, category) - Get the task statistics for the given category.\n    self - Reference to the current manager object.\n    category - A category name. For example: s = q.stats_category(\"my_category\") >>> print(s)  The fields in ndcctools.taskvine.manager.Manager.stats can also be individually accessed through this call. For example: >>> print(s.tasks_waiting)\n  status(self, request) - Get manager information as list of dictionaries.\n    self - Reference to the current manager object\n    request - One of: \"manager\", \"tasks\", \"workers\", or \"categories\" For example: import json tasks_info = q.status(\"tasks\")\n  summarize_workers(self) - Get resource statistics of workers connected.\n    self - Reference to the current manager object.\n  update_catalog(self) - Send update to catalog server.\n    self - Reference to the current manager object.\n  set_category_mode(self, category, mode) - Turn on or off first-allocation labeling for a given category.\n    self - Reference to the current manager object.\n    category - A category name. If None, sets the mode by default for newly created categories.\n    mode - One of: \"fixed\" Task fails (default). \"max\" If maximum values are specified for cores, memory, disk, and gpus (e.g. via ndcctools.taskvine.manager.Manager.set_category_resources_max or ndcctools.taskvine.task.Task.set_memory), and one of those resources is exceeded, the task fails. Otherwise it is retried until a large enough worker connects to the manager, using the maximum values specified, and the maximum values so far seen for resources not specified. Use ndcctools.taskvine.task.Task.set_retries to set a limit on the number of times manager attemps to complete the task. \"min waste\" As above, but manager tries allocations to minimize resource waste. \"max throughput\" As above, but manager tries allocations to maximize throughput.\n  set_category_autolabel_resource(self, category, resource, autolabel) - Turn on or off first-allocation labeling for a given category and resource.\n    self - Reference to the current manager object.\n    category - A category name.\n    resource - A resource name.\n    autolabel - True/False for on/off.\n  task_state() - Get current task state.\n  enable_monitoring(self, watchdog, time_series) - Enables resource monitoring for tasks.\n    self - Reference to the current manager object.\n    watchdog - If not 0, kill tasks that exhaust declared resources.\n    time_series - If not 0, generate a time series of resources per task in VINE_RUNTIME_INFO_DIR/vine-logs/time-series/ (WARNING: for long running tasks these files may reach gigabyte sizes. This function is mostly used for debugging.)\n  enable_peer_transfers(self) - Enable P2P worker transfer functionality.\n    self - Reference to the current manager object.\n  disable_peer_transfers(self) - Disable P2P worker transfer functionality.\n    self - Reference to the current manager object.\n  enable_disconnect_slow_workers(self) - Change the project name for the given manager.\n    self - Reference to the current manager object.\n  enable_disconnect_slow_workers_category(self, name, multiplier) - Enable disconnect slow workers functionality for a given manager.\n    self - Reference to the current manager object.\n    name - Name of the category.\n    multiplier - The multiplier of the average task time at which point to disconnect a worker; disabled if less than one (see ndcctools.taskvine.manager.Manager.enable_disconnect_slow_workers)\n  set_draining_by_hostname(self, hostname, drain_mode) - Turn on or off draining mode for workers at hostname.\n    self - Reference to the current manager object.\n    hostname - The hostname the host running the workers.\n    drain_mode - If True, no new tasks are dispatched to workers at hostname, and empty workers are shutdown. Else, workers works as usual.\n  empty(self) - Determine whether there are any known tasks managerd, running, or waiting to be collected.\n    self - Reference to the current manager object.\n  hungry(self) - Determine whether the manager can support more tasks.\n    self - Reference to the current manager object.\n  set_scheduler(self, scheduler) - Set the worker selection scheduler for manager.\n    self - Reference to the current manager object.\n    scheduler - One of the following schedulers to use in assigning a task to a worker. See vine_schedule_t for possible values.\n  set_name(self, name) - Change the project name for the given manager.\n    self - Reference to the current manager object.\n    name - The new project name.\n  set_manager_preferred_connection(self, mode) - Set the preference for using hostname over IP address to connect.\n    self - Reference to the current manager object.\n    mode - An string to indicate using 'by_ip', 'by_hostname' or 'by_apparent_ip'.\n  set_min_task_id(self, minid) - Set the minimum task_id of future submitted tasks.\n    self - Reference to the current manager object.\n    minid - Minimum desired task_id\n  set_priority(self, priority) - Change the project priority for the given manager.\n    self - Reference to the current manager object.\n    priority - An integer that presents the priorty of this manager manager. The higher the value, the higher the priority.\n  tasks_left_count(self, ntasks) - Specify the number of tasks not yet submitted to the manager.\n    self - Reference to the current manager object.\n    ntasks - Number of tasks yet to be submitted.\n  set_catalog_servers(self, catalogs) - Specify the catalog servers the manager should report to.\n    self - Reference to the current manager object.\n    catalogs - The catalog servers given as a comma delimited list of hostnames or hostname:port\n  set_property(m, name, value) - Add a global property to the manager which will be included in periodic reports to the catalog server and other telemetry destinations.\n    m - A manager object\n    name - The name of the property.\n    value - The value of the property.\n  set_runtime_info_path(self, dirname) - Specify a directory to write logs and staging files.\n    self - Reference to the current manager object.\n    dirname - A directory name\n  set_password(self, password) - Add a mandatory password that each worker must present.\n    self - Reference to the current manager object.\n    password - The password.\n  set_password_file(self, file) - Add a mandatory password file that each worker must present.\n    self - Reference to the current manager object.\n    file - Name of the file containing the password.\n  set_resources_max(self, rmd) - Specifies the maximum resources allowed for the default category.\n    self - Reference to the current manager object.\n    rmd - Dictionary indicating maximum values. See ndcctools.taskvine.task.Task.resources_measured for possible fields. For example: >>> # A maximum of 4 cores is found on any worker: >>> q.set_resources_max({'cores': 4}) >>> # A maximum of 8 cores, 1GB of memory, and 10GB disk are found on any worker: >>> q.set_resources_max({'cores': 8, 'memory':  1024, 'disk': 10240})\n  set_resources_min(self, rmd) - Specifies the minimum resources allowed for the default category.\n    self - Reference to the current manager object.\n    rmd - Dictionary indicating minimum values. See ndcctools.taskvine.task.Task.resources_measured for possible fields. For example: >>> # A minimum of 2 cores is found on any worker: >>> q.set_resources_min({'cores': 2}) >>> # A minimum of 4 cores, 512MB of memory, and 1GB disk are found on any worker: >>> q.set_resources_min({'cores': 4, 'memory':  512, 'disk': 1024})\n  set_category_resources_max(self, category, rmd) - Specifies the maximum resources allowed for the given category.\n    self - Reference to the current manager object.\n    category - Name of the category.\n    rmd - Dictionary indicating maximum values. See ndcctools.taskvine.task.Task.resources_measured for possible fields. For example: >>> # A maximum of 4 cores may be used by a task in the category: >>> q.set_category_resources_max(\"my_category\", {'cores': 4}) >>> # A maximum of 8 cores, 1GB of memory, and 10GB may be used by a task: >>> q.set_category_resources_max(\"my_category\", {'cores': 8, 'memory':  1024, 'disk': 10240})\n  set_category_resources_min(self, category, rmd) - Specifies the minimum resources allowed for the given category.\n    self - Reference to the current manager object.\n    category - Name of the category.\n    rmd - Dictionary indicating minimum values. See ndcctools.taskvine.task.Task.resources_measured for possible fields. For example: >>> # A minimum of 2 cores is found on any worker: >>> q.set_category_resources_min(\"my_category\", {'cores': 2}) >>> # A minimum of 4 cores, 512MB of memory, and 1GB disk are found on any worker: >>> q.set_category_resources_min(\"my_category\", {'cores': 4, 'memory':  512, 'disk': 1024})\n  set_category_first_allocation_guess(self, category, rmd) - Specifies the first-allocation guess for the given category.\n    self - Reference to the current manager object.\n    category - Name of the category.\n    rmd - Dictionary indicating maximum values. See ndcctools.taskvine.task.Task.resources_measured for possible fields. For example: >>> # Tasks are first tried with 4 cores: >>> q.set_category_first_allocation_guess(\"my_category\", {'cores': 4}) >>> # Tasks are first tried with 8 cores, 1GB of memory, and 10GB: >>> q.set_category_first_allocation_guess(\"my_category\", {'cores': 8, 'memory':  1024, 'disk': 10240})\n  set_category_max_concurrent(self, category, max_concurrent) - Specifies the maximum resources allowed for the given category.\n    self - Reference to the current work queue object.\n    category - Name of the category.\n    max_concurrent - Number of maximum concurrent tasks. Less then 0 means unlimited (this is the default). For example: >>> # Do not run more than 5 tasks of \"my_category\" concurrently: >>> q.set_category_max_concurrent(\"my_category\", 5)\n  initialize_categories(self, rm, filename) - Initialize first value of categories.\n    self - Reference to the current manager object.\n    rm - Dictionary indicating maximum values. See ndcctools.taskvine.task.Task.resources_measured for possible fields.\n    filename - JSON file with resource summaries.\n  cancel_by_task_id(self, id) - Cancel task identified by its task_id.\n    self - Reference to the current manager object.\n    id - The task_id returned from ndcctools.taskvine.manager.Manager.submit.\n  cancel_by_task_tag(self, tag) - Cancel task identified by its tag.\n    self - Reference to the current manager object.\n    tag - The tag assigned to task using ndcctools.taskvine.task.Task.set_tag.\n  cancel_by_category(self, category) - Cancel all tasks of the given category.\n    self - Reference to the current manager object.\n    category - The name of the category to cancel.\n  cancel_all(self) - Cancel all tasks.\n    self - Reference to the current manager object.\n  workers_shutdown(self, n) - Shutdown workers connected to manager.\n    self - Reference to the current manager object.\n    n - The number to shutdown. 0 shutdowns all workers\n  block_host(self, host) - Block workers running on host from working for the manager.\n    self - Reference to the current manager object.\n    host - The hostname the host running the workers.\n  blacklist() - Replaced by ndcctools.taskvine.manager.Manager.block_host.\n  block_host_with_timeout(self, host, timeout) - Block workers running on host for the duration of the given timeout.\n    self - Reference to the current manager object.\n    host - The hostname the host running the workers.\n    timeout - How long this block entry lasts (in seconds). If less than 1, block indefinitely.\n  blacklist_with_timeout() - See ndcctools.taskvine.manager.Manager.block_host_with_timeout.\n  unblock_host(self, host) - Unblock given host, of all hosts if host not given.\n    self - Reference to the current manager object.\n    host - The of the hostname the host.\n  blacklist_clear() - See ndcctools.taskvine.manager.Manager.unblock_host.\n  set_keepalive_interval(self, interval) - Change keepalive interval for a given manager.\n    self - Reference to the current manager object.\n    interval - Minimum number of seconds to wait before sending new keepalive checks to workers.\n  set_keepalive_timeout(self, timeout) - Change keepalive timeout for a given manager.\n    self - Reference to the current manager object.\n    timeout - Minimum number of seconds to wait for a keepalive response from worker before marking it as dead.\n  tune(self, name, value) - Tune advanced parameters.\n    self - Reference to the current manager object.\n    name - The name fo the parameter to tune. Can be one of following: \"attempt-schedule-depth\" The amount of tasks to attempt scheduling on each pass of send_one_task in the main loop. (default=100) \"category-steady-n-tasks\" Set the number of tasks considered when computing category buckets. \"default-transfer-rate\" The assumed network bandwidth used until sufficient data has been collected. (1MB/s) \"disconnect-slow-workers-factor\" Set the multiplier of the average task time at which point to disconnect a worker; disabled if less than 1. (default=0) \"hungry-minimum\" Mimimum number of tasks to consider manager not hungry. (default=10) \"hungry-minimum-factor\" Queue is hungry if number of waiting tasks is less than hungry-minumum-factor x (number of workers) | 2 | \"immediate-recovery\" If set to 1, create recovery tasks for temporary files as soon as their worker disconnects. Otherwise, create recovery tasks only if the temporary files are used as input when trying to dispatch another task. \"keepalive-interval\" Set the minimum number of seconds to wait before sending new keepalive checks to workers. (default=300) \"keepalive-timeout\" Set the minimum number of seconds to wait for a keepalive response from worker before marking it as dead. (default=30) \"long-timeout\" Set the minimum timeout in seconds when sending a large message to a single worker. (default=3600) \"max-retrievals\" Sets the max number of tasks to retrieve per manager wait(). If less than 1, the manager prefers to retrieve all completed tasks before dispatching new tasks to workers. (default=1) \"min-transfer-timeout\" Set the minimum number of seconds to wait for files to be transferred to or from a worker. (default=10) \"monitor-interval\" Parameter to change how frequently the resource monitor records resource consumption of a task in a times series, if this feature is enabled. See enable_monitoring. \"prefer-dispatch\" If 1, try to dispatch tasks even if there are retrieved tasks ready to be reported as done. (default=0) \"proportional-resources\" If set to 0, do not assign resources proportionally to tasks. The default is to use proportions. \"proportional-whole-tasks\" Round up resource proportions such that only an integer number of tasks could be fit in the worker. The default is to use proportions. \"ramp-down-heuristic\" If set to 1 and there are more workers than tasks waiting, then tasks are allocated all the free resources of a worker large enough to run them. If monitoring watchdog is not enabled, then this heuristic has no effect. (default=0) \"resource-submit-multiplier\" Treat each worker as having ({cores,memory,gpus} * multiplier) when submitting tasks. This allows for tasks to wait at a worker rather than the manager. (default = 1.0) \"short-timeout\" Set the minimum timeout when sending a brief message to a single worker. (default=5s) \"transfer-outlier-factor\" Transfer that are this many times slower than the average will be terminated. (default=10x) \"transfer-replica-per-cycle\" Number of replicas to schedule per file per iteration. (default=1) \"transfer-temps-recovery\" If 1, try to replicate temp files to reach threshold on worker removal. (default=0) \"transient-error-interval\" Time to wait in seconds after a resource failure before attempting to use it again. (default=15) \"wait-for-workers\" Mimimum number of workers to connect before starting dispatching tasks. (default=0) \"wait-retrieve-many\" If set to 0, cvine.vine_wait breaks out of the while loop whenever a task changes to \"task_done\" (wait_retrieve_one mode). If set to 1, vine_wait does not break, but continues recieving and dispatching tasks. This occurs until no task is sent or recieved, at which case it breaks out of the while loop (wait_retrieve_many mode). (default=0) \"worker-retrievals\" If 1, retrieve all completed tasks from a worker when retrieving results, even if going above the parameter max-retrievals . Otherwise, if 0, retrieve just one task before deciding to dispatch new tasks or connect new workers. (default=1) \"watch-library-logfiles\" If 1, watch the output files produced by each of the library processes running on the remote workers, take them back the current logging directory. (default=0)\n    value - The value to set the parameter to.\n  submit(self, task) - Submit a task to the manager.\n    self - Reference to the current manager object.\n    task - A task description created from ndcctools.taskvine.task.Task.\n  install_library(self, task) - Submit a library to install on all connected workers.\n    self - Reference to the current manager object.\n    task - A Library Task description created from create_library_from_functions or create_library_from_files\n  remove_library(self, name) - Remove a library from all connected workers.\n    self - Reference to the current manager object.\n    name - Name of the library to be removed.\n  check_library_exists(self, library_name) - Check whether a libray exists on the manager or not.\n    self - Reference to the current manager object.\n    library_name - Name of the library to be checked\n  create_library_from_functions(self, library_name, function_list, poncho_env, init_command, add_env, hoisting_modules, exec_mode, library_context_info) - Turn a list of python functions into a Library Task.\n    self - Reference to the current manager object.\n    library_name - Name of the Library to be created\n    function_list - List of all functions to be included in the library\n    poncho_env - Name of an already prepared poncho environment or a conda environment\n    init_command - A string describing a shell command to execute before the library task is run\n    add_env - Whether to automatically create and/or add environment to the library\n    hoisting_modules - A list of modules imported at the preamble of library, including packages, functions and classes.\n    exec_mode - Execution mode that the library should use to run function calls. Either 'direct' or 'fork'\n    library_context_info - A list containing [library_context_func, library_context_args, library_context_kwargs]. Used to create the library context on remote nodes.\n  create_library_from_serverized_files(self, library_name, library_path, env) - Turn Library code created with poncho_package_serverize into a Library Task.\n    self - Reference to the current manager object.\n    library_name - Name that identifies this library to the FunctionCalls\n    library_path - Filename of the library (i.e., the output of poncho_package_serverize)\n    env - Environment to run the library. Either a vine file that expands to an environment (see ndcctools.taskvine.task.Task.add_environment), or a path to a poncho environment.\n  create_library_from_command(self, executable_path, name, env) - Create a Library task from arbitrary inputs.\n    self - Reference to the current manager object\n    executable_path - Filename of the library executable\n    name - Name of the library to be created\n    env - Environment to run the library. Either a vine file that expands to an environment (see ndcctools.taskvine.task.Task.add_environment), or a path to a poncho environment.\n  wait(self, timeout) - Wait for tasks to complete.\n    self - Reference to the current manager object.\n    timeout - The number of seconds to wait for a completed task before returning. Use an integer to set the timeout or the value \"wait_forever\" to block until a task has completed. If 0, return immediately with a complete task if one available, or None otherwise.\n  wait_for_tag(self, tag, timeout) - Similar to ndcctools.taskvine.manager.Manager.wait, but guarantees that the returned task has the specified tag.\n    self - Reference to the current manager object.\n    tag - Desired tag. If None, then it is equivalent to self.wait(timeout)\n    timeout - The number of seconds to wait for a completed task before returning. If 0, return immediately with a complete task if one available, or None otherwise.\n  wait_for_task_id(self, task_id, timeout) - Similar to ndcctools.taskvine.manager.Manager.wait, but guarantees that the returned task has the specified task_id.\n    self - Reference to the current manager object.\n    task_id - Desired task_id. If -1, then it is equivalent to self.wait(timeout)\n    timeout - The number of seconds to wait for a completed task before returning. If 0, return immediately with a complete task if one available, or None otherwise.\n  application_info(self) - Should return a dictionary with information for the status display.\n    self - Reference to the current work queue object.\n  map(self, fn, seq, chunksize) - Maps a function to elements in a sequence using taskvine.\n    self - Reference to the current manager object.\n    fn - The function that will be called on each element\n    seq - The sequence that will call the function\n    chunksize - The number of elements to process at once\n  pair(self, fn, seq1, seq2, chunksize, env) - Returns the values for a function of each pair from 2 sequences.\n    self - Reference to the current manager object.\n    fn - The function that will be called on each element\n    seq1 - The first seq that will be used to generate pairs\n    seq2 - The second seq that will be used to generate pairs\n    chunksize - Number of pairs to process at once (default is 1)\n    env - Filename of a python environment tarball (conda or poncho)\n  tree_reduce(self, fn, seq, chunksize) - Reduces a sequence until only one value is left, and then returns that value.\n    self - Reference to the current manager object.\n    fn - The function that will be called on each element\n    seq - The seq that will be reduced\n    chunksize - The number of elements per Task (for tree reduc, must be greater than 1)\n  remote_map(self, fn, seq, library, name, chunksize) - Maps a function to elements in a sequence using taskvine remote task.\n    self - Reference to the current manager object.\n    fn - The function that will be called on each element. This function exists in library.\n    seq - The sequence that will call the function\n    library - The name of the library that contains the function fn.\n    name - This defines the key in the event json that wraps the data sent to the library.\n    chunksize - The number of elements to process at once\n  remote_pair(self, fn, seq1, seq2, library, name, chunksize) - Returns the values for a function of each pair from 2 sequences using remote task.\n    self - Reference to the current manager object.\n    fn - The function that will be called on each element. This function exists in library.\n    seq1 - The first seq that will be used to generate pairs\n    seq2 - The second seq that will be used to generate pairs\n    library - The name of the library that contains the function fn.\n    name - This defines the key in the event json that wraps the data sent to the library.\n    chunksize - The number of elements to process at once\n  remote_tree_reduce(self, fn, seq, library, name, chunksize) - Reduces a sequence until only one value is left, and then returns that value.\n    self - Reference to the current manager object.\n    fn - The function that will be called on each element. Exists on the library\n    seq - The seq that will be reduced\n    library - The name of the library that contains the function fn.\n    name - This defines the key in the event json that wraps the data sent to the library.\n    chunksize - The number of elements per Task (for tree reduc, must be greater than 1)\n  declare_file(self, path, cache, peer_transfer, unlink_when_done) - Declare a file obtained from the local filesystem.\n    self - The manager to register this file\n    path - The path to the local file\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n    unlink_when_done - Whether to delete the file when its reference count is 0. (Warning: Only use on files produced by the application, and never on irreplaceable input files.)\n  fetch_file(self, file) - Fetch file contents from the cluster or local disk.\n    self - The manager to register this file\n    file - The file object\n  undeclare_file(self, file) - Un-declare a file that was created by declare_file or similar methods.\n    self - The manager to register this file\n    file - The file object\n  undeclare_function(self, fn) - Remove the manager's local serialized copy of a function used with PythonTask.\n    self - The manager to register this file\n    fn - The function that the manager should forget.\n  declare_temp(self) - Declare an anonymous file has no initial content, but is created as the output of a task, and may be consumed by other tasks.\n    self - The manager to register this file\n  declare_url(self, url, cache, peer_transfer) - Declare a file obtained from a remote URL.\n    self - The manager to register this file\n    url - The url of the file.\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  declare_buffer(self, buffer, cache, peer_transfer) - Declare a file created from a buffer in memory.\n    self - The manager to register this file\n    buffer - The contents of the buffer, or None for an empty output buffer\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  declare_minitask(self, minitask, source, cache, peer_transfer) - Declare a file created by executing a mini-task.\n    self - The manager to register this file\n    minitask - The task to execute in order to produce a file\n    source - The name of the file to extract from the task's sandbox.\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  declare_untar(self, tarball, cache, peer_transfer) - Declare a file created by by unpacking a tar file.\n    self - The manager to register this file\n    tarball - The file object to un-tar\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  declare_poncho(self, package, cache, peer_transfer) - Declare a file that sets up a poncho environment.\n    self - The manager to register this file\n    package - The poncho environment tarball. Either a vine file or a string representing a local file.\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  declare_starch(self, starch, cache, peer_transfer) - Declare a file create a file by unpacking a starch package.\n    self - The manager to register this file\n    starch - The startch .sfx file. Either a vine file or a string representing a local file.\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  declare_xrootd(self, source, proxy, env, cache, peer_transfer) - Declare a file from accessible from an xrootd server.\n    self - The manager to register this file.\n    source - The URL address of the root file in text form as: \"root://XROOTSERVER[:port]//path/to/file\"\n    proxy - A ndcctools.taskvine.file.File of the X509 proxy to use. If None, the environment variable X509_USER_PROXY and the file \"$TMPDIR/$UID\" are considered in that order. If no proxy is present, the transfer is tried without authentication.\n    env - If not None, an environment file (e.g poncho or starch, see ndcctools.taskvine.task.Task.add_environment) that contains the xrootd executables. Otherwise assume xrootd is available at the worker.\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  declare_chirp(self, server, source, ticket, env, cache, peer_transfer) - Declare a file from accessible from an xrootd server.\n    self - The manager to register this file.\n    server - The chirp server address of the form \"hostname[:port\"]\"\n    source - The name of the file in the server\n    ticket - If not None, a file object that provides a chirp an authentication ticket\n    env - If not None, an environment file (e.g poncho or starch) that contains the chirp executables. Otherwise assume chirp is available at the worker.\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'worker', the file is cache until the end-of-life of the worker. If 'forever', the file is cached beyond the end-of-life of the worker. Default is False (file is not cached).\n    peer_transfer - Whether the file can be transfered between workers when peer transfers are enabled (see ndcctools.taskvine.manager.Manager.enable_peer_transfers). Default is True.\n  log_txn_app(self, server) - Adds a custom APPLICATION entry to the transactions log.\n    self - The manager to register this file.\n    server - A custom transaction message\n  log_debug_app(self, server) - Adds a custom APPLICATION entry to the debug log.\n    self - The manager to register this file.\n    server - A custom debug message\nndcctools.taskvine.task.Task\n  __init__(self, command, task_info) - Create a new task specification.\n    self - Reference to the current task object.\n    command - The shell command line to be exected by the task.\n    task_info - Optional dictionary containing specified task parameters.\n  submit_finalize(self) - Finalizes the task definition once the manager that will execute is run.\n    self - Reference to the current python task object\n  clone() - Return a copy of this task.\n  set_command(self, command) - Set the command to be executed by the task.\n    self - Reference to the current task object.\n    command - The command to be executed.\n  set_library_required(self, library) - Set the name of the library at the worker that should execute the task's command.\n    self - Reference to the current task object.\n    library - The library or the name of the library\n  get_library_required(self) - Get the name of the library at the worker that should execute the task's command.\n    self - Reference to the current task object.\n  needs_library() - Deprecated, see set_library_required.\n  set_library_provided(self, library_name) - Set the library name provided by this task.\n    self - Reference to the current task object.\n    library_name - The name of the library.\n  get_libray_provided(self) - Get the name of the library at the worker that should execute the task's command.\n    self - Reference to the current task object.\n  provides_library() - Deprecated, see set_library_provided.\n  set_function_slots(self, nslots) - Set the number of concurrent functions a library can run.\n    self - Reference to the current task object.\n    nslots - The maximum number of concurrent functions this library can run.\n  set_function_exec_mode_from_string(self, exec_mode) - Set the execution mode of functions in a library.\n    self - Reference to the current task object.\n    exec_mode - The execution mode of functions in a library. Either 'fork' or 'direct'.\n  set_scheduler(self, scheduler) - Set the worker selection scheduler for task.\n    self - Reference to the current task object.\n    scheduler - One of the following schedulers to use in assigning a task to a worker. See vine_schedule_t for possible values.\n  set_tag(self, tag) - Attach a user defined logical name to the task.\n    self - Reference to the current task object.\n    tag - The tag to attach to task.\n  set_category(self, name) - Label the task with the given category.\n    self - Reference to the current task object.\n    name - The name of the category\n  add_feature(self, name) - Label the task with the given user-defined feature.\n    self - Reference to the current task object.\n    name - The name of the feature.\n  add_input(self, file, remote_name, strict_input) - Add any input object to a task.\n    self - Reference to the current task object.\n    file - A file object of class ndcctools.taskvine.file.File, such as from ndcctools.taskvine.manager.Manager.declare_file, ndcctools.taskvine.manager.Manager.declare_buffer, ndcctools.taskvine.manager.Manager.declare_url, etc.\n    remote_name - The name of the file at the execution site.\n    strict_input - Whether the file should be transfered to the worker for execution. If no worker has all the input files already cached marked as strict inputs for the task, the task fails.\n  add_output(self, file, remote_name, watch, success_only, failure_only) - Add any output object to a task.\n    self - Reference to the current task object.\n    file - A file object of class ndcctools.taskvine.file.File, such as from ndcctools.taskvine.manager.Manager.declare_file, or ndcctools.taskvine.manager.Manager.declare_buffer ndcctools.taskvine.task.Task.add_input\n    remote_name - The name of the file at the execution site.\n    watch - Watch the output file and send back changes as the task runs.\n    success_only - Whether the file should be retrieved only when the task succeeds. Default is False.\n    failure_only - Whether the file should be retrieved only when the task fails (e.g., debug logs). Default is False.\n  set_snapshot_file(self, filename) - When monitoring, indicates a json-encoded file that instructs the monitor to take a snapshot of the task resources.\n    self - Reference to the current task object.\n    filename - The name of the snapshot events specification\n  add_starch_package(t, f) - Add a Starch package as an execution context.\n    t - A task object.\n    f - A file containing an unpacked Starch package.\n  add_poncho_package(t, f) - Add a Poncho package as an execution context.\n    t - A task object.\n    f - A file containing an unpacked Poncho package.\n  add_execution_context(t, f) - Adds an execution context to the task.\n    t - A task object.\n    f - The execution context file.\n  set_retries() - Indicate the number of times the task should be retried.\n  set_max_forsaken() - Indicate the number of times the task can be returned to the manager without being executed.\n  set_cores() - Indicate the number of cores required by this task.\n  set_memory() - Indicate the memory (in MB) required by this task.\n  set_disk() - Indicate the disk space (in MB) required by this task.\n  set_gpus() - Indicate the number of GPUs required by this task.\n  set_priority() - Indicate the the priority of this task (larger means better priority, default is 0).\n  set_env_var() - Set this environment variable before running the task.\n  set_monitor_output() - Set a name for the resource summary output directory from the monitor.\n  tag() - Get the user-defined logical name for the task.\n  category() - Get the category name for the task.\n  command() - Get the shell command executed by the task.\n  state() - Get the state of the task.\n  std_output() - Get the standard output of the task.\n  output() - Get the standard output of the task.\n  id() - Get the task id number.\n  exit_code() - Get the exit code of the command executed by the task.\n  result() - Return a string that explains the result of a task.\n  completed() - Return True if task executed and its command terminated normally.\n  successful() - Return True if task executed successfully, (i.e.\n  get_metric() - Return various integer performance metrics about a completed task.\n  addrport() - Get the address and port of the host on which the task ran.\n  hostname() - Get the address and port of the host on which the task ran.\n  resources_measured() - Get the resources measured for the task execution if resource monitoring is enabled.\n  limits_exceeded() - Get the resources the task exceeded.\n  resources_requested() - Get the resources the task requested to run.\n  resources_allocated() - Get the resources allocated to the task in its latest attempt.\n  add_nopen() - Adds inputs for nopen library and rules file and sets LD_PRELOAD.\nndcctools.taskvine.task.LibraryTask\n  __init__(self, fn, library_name) - Create a new LibraryTask task specification.\n    self - Reference to the current remote task object.\n    fn - The command for this LibraryTask to run\n    library_name - The name of this Library.\nndcctools.taskvine.task.PythonTask\n  __init__(self, func, args, kwargs) - Creates a new python task.\n    self - Reference to the current python task object\n    func - python function to be executed by task\n    args - arguments used in function to be executed by task\n    kwargs - keyword arguments used in function to be executed by task\n  submit_finalize(self, manager) - Finalizes the task definition once the manager that will execute is run.\n    self - Reference to the current python task object\n    manager - Manager to which the task was submitted\n  enable_temp_output(self) - Marks the output of this task to stay at the worker.\n    self - Reference to the current python task object\n  set_output_cache(cache) - Set the cache behavior for the output of the task.\n    cache - If True or 'workflow', cache the file at workers for reuse until the end of the workflow. If 'always', the file is cache until the end-of-life of the worker. Default is False (file is not cache).\n  output_file() - Returns the ndcctools.taskvine.file.File object that represents the output of this task.\n  output(self) - returns the result of a python task as a python variable\n    self - reference to the current python task object\n  disable_output_serialization(self) - Disables serialization of results to disk when writing to a file for transmission.\n    self - Reference to the current python task object\nndcctools.taskvine.task.FunctionCall\n  __init__(self, library, fn, args, kwargs) - Create a new FunctionCall specification.\n    self - Reference to the current FunctionCall object.\n    library - The library, or name of the library which has the function you wish to execute.\n    fn - The name of the function to be executed on the library.\n    args - positional arguments used in function to be executed by task. Can be mixed with kwargs\n    kwargs - keyword arguments used in function to be executed by task.\n  submit_finalize(self) - Finalizes the task definition once the manager that will execute is run.\n    self - Reference to the current python task object\n  set_fn_args(self, args, kwargs) - Specify function arguments.\n    self - Reference to the current remote task object\n    args - An array of positional args to be passed to the function\n    kwargs - A dictionary of keyword arguments to be passed to the function\n  set_exec_method(self, remote_task_exec_method) - Specify how the remote task should execute.\n    self - Reference to the current remote task object\n    remote_task_exec_method - Can be either of \"fork\" or \"direct\". Fork creates a child process to execute the function and direct has the worker directly call the function.\n  output() - Retrieve output, handles cleanup, and returns result or failure reason.\n"], "prompts": [{"id": "taskvine0", "content": "Write a Python TaskVine application to get the current date on a worker machine. The date must be stored to an output file named \"current_date\"."}, {"id": "taskvine1", "content": "Write a Python TaskVine application to count the number of times the keywords \"wealth\", \"nation\", \"labour\", \"price\", \"nature\", and \"commodity\" appear in Adam Smith's Wealth of Nations. The text is available at \"https://www.gutenberg.org/cache/epub/3300/pg3300.txt\"."}, {"id": "taskvine2", "content": "Write a Python TaskVine application to run my program \"script.sh\" with various input arguments and organizes all output data. \"script.sh\" takes, as input, an integer and two files. \"script.sh\" outputs a file named \"output{i}.txt\" where \"{i}\" is the input integer. The application must run \"script.sh\" with input integers 1-1000 and input files \"file1.input\" and \"file2.input\" which are stored in the directory \"script_inputs\". The application must store all output files in a directory named \"script_outputs\"."}, {"id": "taskvine3", "content": "Write a Python TaskVine application to execute a pipeline of shell scripts. I have the script \"script1.sh\" which outputs a file named \"intermediary.out\" upon completion. I also have the scripts \"script2.sh\" and \"script3.sh\"  which both accept an input file as a command line argument. \"script2.sh\" and \"script3.sh\" must both receive \"intermediary.out\" as the input. Finally, \"script2.sh\" and \"script3.sh\" output files named \"script2.out\" and \"script3.out\", respectively. Save these outputs files on the host machine. \"intermediary.out\" need not be saved to the host machine."}, {"id": "taskvine4", "content": "Write a Python TaskVine application to run my GPU-accelerated program. The program binary is stored at \"./train_demo\". The program requires that the worker machine has the feature \"NVIDIA RTX 4090\". Upon completion, the program outputs \"model.tar.gz\". The output file must be saved to the host machine."}, {"id": "taskvine5", "content": "Write a Python TaskVine application to compile and test my project stored at \"complex_project/\". To compile the project, run the \"make all\" command in the project's root directory. Then, to test the project, run \"make test\". The compilation and test must use at most 4 cores, 1024 MB of memory, and 2048 MB of disk. Enable resource monitoring to track resource usage, but do not terminate the task if it overuses resources. When the task completes, print the task's exit code and standard output. For each resource that the task overused, print the resource and by how much it overused."}, {"id": "taskvine6", "content": "Write a Python TaskVine application that executes multiple programs with different levels of resource requirements. The two task categories must be named \"intensive\" and \"light\". The \"intensive\" tasks require 8 cores and 16 GB of memory. The \"light\" tasks require 1 core and 2 GB of memory. In the intensive category, submit 10 tasks to execute the shell script \"intensive.sh\". In the light category, submit 100 tasks to execute the shell script \"light.sh\" Print the standard output of each task upon completion."}, {"id": "taskvine7", "content": "Write a Python TaskVine application to perform matrix inversion and transposition. The matrix computations must be defined in Python functions and added to a TaskVine library called \"matrix_library\". The user must specify the size, m x n,  of the matrix and the number of matrices, i, on which to perform computation. The application must randomly generate i matrices and for each matrix, print the matrix, print the inverse matrix, and print the transposed matrix."}, {"id": "taskvine8", "content": "Write a Python Taskvine application to fetch the titles of multiple news webpages. The webpage contents must be downloaded asynchronously using the TaskVine futures executor. Print the URL and title of each webpage in order of the completion of each task. The webpages I am interested in are \"https://abcnews.com\", \"https://apnews.com\", \"https://axios.com\", \"https://cnn.com\", \"https://nbcnews.com\", \"https://nytimes.com\", \"https://reuters.com\", \"https://washingtonpost.com\", \"https://wsj.com\"."}, {"id": "taskvine9", "content": "Write a Python TaskVine application to get the current date on a worker machine. The date must be stored to an output file named \"current_date\". To prevent an unwanted worker from connecting to the manager, use the password stored in the file \"taskvine.password\". Further, encrypt communication between the manager and worker with SSL using the \"key.pem\" and \"certificate.pem\" files."}]}